---
title: "Introduction To Empirical Bayes Notes."
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
renv::restore()
r_packages <- c("ggplot2", "dplyr", "tidyr", "purrr")
renv::install(r_packages)

library(dplyr)
library(tidyr)
library(ggplot2)
library(purrr)
ggplot2::theme_set(ggplot2::theme_bw())
```

Notes from [Introduction to Empirical Bayes: Examples from Baseball Statistics](http://varianceexplained.org/r/empirical-bayes-book/) by David Robinson

## Why Empirical Bayes?

Empirical Bayesian methods are an approximation to more exact methods but come with some controversy. However, EB methods are well suited for Data Science because

- Though EB is Inaccurate with few observations, with large datasets performance can be comparable to traditional Bayesian methods
- EB provides shortcuts that allow easy computation at scale; Approximate in a fraction of full Bayesian Methods time.

We can use EB as a fast approximation technique when we have many observations.

## 2 The beta distribution

The _beta distribtion_ is a probability distribution with two Parameters $\alpha$ (alpha) and $\beta$ (Beta).  The distribution is constrained between 0 and 1.  In practice, it is good at representing a probability distribution of probabilities.

```{r, echo = FALSE, assorted-betas}
size = 1e6
tibble::tibble(
  alpha = c(1, 3, 20, 50),
  beta  = c(2, 3, 20, 10),
) %>% dplyr::mutate(
  batting_average  = purrr::map2(alpha, beta, ~ rbeta(size, .x, .y)),
  Parameters = paste0("alpha = ", alpha, " beta = ", beta)
) %>% 
  tidyr::unnest(batting_average) %>%
  ggplot2::ggplot(ggplot2::aes(x = batting_average, color = Parameters)) + 
  ggplot2::geom_density() + 
  ggplot2::xlab("Batting Average") +
  ggplot2::ylab("Density of beta") + 
  theme_bw()
```

### 2.1 The Binomial Distribution

The _binomial distribution_ models a count of successes out of the total.  To follow the baseball analogy, for 100 at-bats, we would say the number of hits a player will get is distributed according to `Binomial(100, p)` where `p` is the probability that at-bat `AB` will be a hit `H.`  In the baseball case, the `p` the thing we want estimate is a batting average.

> Batting Average = Hits / At Bats

### 2.2 Updating The Beta Distribution

We can build an existing belief (prior) regarding what a reasonable batting average should be.  The text suggests that belief is 0.27, with an alpha = 81, and Beta of 219.  However, a batting average could reasonably range from 0.21 to 0.35 (see plot below).  So if we start with this existing belief of where any players true batting average _could be_, how do we update our prior beliefs based on new information as the player gets at-bats in.

```{r, echo = FALSE, beta_for_batting_averages}
size = 1e6
tibble::tibble(
  alpha = c(81),
  beta  = c(219),
) %>% dplyr::mutate(
  batting_average  = purrr::map2(alpha, beta, ~ rbeta(size, .x, .y)),
  Parameters = paste0("alpha = ", alpha, " beta = ", beta)
) %>% 
  tidyr::unnest(batting_average) %>%
  ggplot2::ggplot(ggplot2::aes(x = batting_average, color = Parameters)) + 
  ggplot2::geom_density() + 
  ggplot2::xlab("Batting Average") +
  ggplot2::ylab("Density of beta") + 
  theme_bw()
```

This idea, `prior expectations + new information = new expectation`, is a simplification of the Bayesian Philosophy.  In the case of updating our beta distribution, the update rule is straightforward.

> Beta(alpha<sub>0</sub> + hits, beta<sub>0</sub> + misses)

So, if a player were to have a hit on their first at-bat, the new Beta would be Beta (82, 219).  When we plot both distributions, we can see that the posterior distribution shifts slightly to the right.  However, the movement is minimal if we were to consider a batter who has hit 200 out of 600 at-bats. The new posterior would be Beta (282, 619), a much more significant shift, since we have collected more evidence for the update rule.

```{r, echo = TRUE, update-the-betas-a-little-and-a-lot}
size = 1e6
tibble::tibble(
  alpha = c(81, 82, 282),
  beta  = c(219, 219, 619),
) %>% dplyr::mutate(
  batting_average  = purrr::map2(alpha, beta, ~ rbeta(size, .x, .y)),
  Parameters = paste0("alpha = ", alpha, " beta = ", beta)
) %>% 
  tidyr::unnest(batting_average) %>%
  ggplot2::ggplot(ggplot2::aes(x = batting_average, color = Parameters)) + 
  ggplot2::geom_density() + 
  ggplot2::xlab("Batting Average") +
  ggplot2::ylab("Density of beta") + 
  theme_bw()
```

#### 2.2.1 Calculating The Posterier Mean

The expected mean of the Beta distribution can be described as:

> alpha / (alpha + beta)

### 2.3 Conjugate Prior

Why is the update so easy?  It is because the beta distribution is the [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior#Example) of the Binomial distribution.  This means there are clear cut rules on updating the prior to get to the posterior distribution. [Wikipedia has a list](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions).  If a prior is not a conjugate prior, then the update can get messy.


Consider a player who got exactly 100 hits in the first 300 at-bats.  What is the true value of their batting average?  We can get a better sense of possible values by simulating many players based on our prior belief regarding the distribution of batting averages we start with Beta (81, 219).


```{r echo=TRUE}
library(dplyr)
num_trials <- 10e6

# Simulate batting statistica based on our existing Prior for our Beta
# Get to a count using the binomial and the true average
simulations <- tibble::tibble(
  true_average = rbeta(num_trials, 81, 219),
  hits = rbinom(num_trials, 300, true_average)
)

# Consider the player who got exactly 100 hits, what is the expected range of their true batting average?
simulations %>% 
            dplyr::filter(hits == 100) %>% 
            ggplot2::ggplot(aes(x=true_average)) + 
                     geom_bar(stat="bin", binwidth = 0.004, ggplot2::aes(y=..density..)) + 
                     geom_density()  + 
                     xlab("Batting average of players who got 100 H / 300 AB.") +
                     ggplot2::theme_bw()
```

What if our player had 60, 80, or 100 hits instead.  The simulations would look like the plot below.  The shapes are similar, but they are shifted to accommodate our new information.  The simulations are not required long term, but helpful for demonstration.  Eventually, we will rely on the update rule "add `hits` to `alpha,` and add `misses` to `beta`"

```{r}
simulations %>%
  filter(hits %in% c(60, 80, 100)) %>% ggplot(aes(true_average, color = factor(hits))) + geom_density() +
  labs(x = "True average of players with H hits / 300 at-bats",
  color = "H") +                
  ggplot2::theme_bw()
```

## 2. Empirical Bayes Estimation

Which sounds higher, `4 out of 10`, or `300 out of 1000`?  Obviously 4/10 = 0.4 and 300/1000 = 0.3.  `0.4` > `0.3` so the answer is clear, but 10 AB is not nearly as much evidence as 1000 AB so that we would have different levels of confidence regarding each.  This sort of hits/attempts analysis is not limited to baseball, consider online advertising for a similar "proportion of success" analysis.

### 2.1 The Lahman baseball dataset

Let's look at real baseball stats wth `Lahman.`


```{r}
library(dplyr) 
library(tidyr) 
library(Lahman)
# Filter out pitchers
career <- Batting %>%
                  filter(AB > 0) %>%
                  anti_join(Pitching, by = "playerID") %>% 
                  group_by(playerID) %>%
                  summarize(H = sum(H), AB = sum(AB)) %>% 
                  mutate(average = H / AB) %>%
                  tibble::as_tibble()

# Include names along with the player IDs
career <- Master %>%
            tibble::as_tibble() %>%
            dplyr::select(playerID, nameFirst, nameLast) %>% 
            unite(name, nameFirst, nameLast, sep = " ") %>% 
            inner_join(career, by = "playerID") %>% 
            dplyr::select(-playerID)

career
```

### 3.2 Estimating A Prior From All Your Data

Our first step of empirical Bayes estimation is to estimate a beta prior, using the data.  Debate exists if an when it is appropriate to use Empirical Bayes methods, but it comes down to how many observations we have.  The less dependent we are on a single observation, the better.  Ultimately, Empirical Bayes is an **approximation** of more exact Bayesian methods.  If we have a lot of data, it is a good approximation.

```{r}
career %>% 
  dplyr::filter(AB > 500) %>%
  ggplot2::ggplot(aes(x=average)) + 
  geom_histogram(bins = 40) +
  ggtitle("Batting Averages of all players with more than 500 At-Bats.")
```

We need to fit a model such that we fix a beta distribution to this dataset.  We can think of the model we want to fit as

`X ~ Beta(alpha0, beta0)` where `alpha0` and `beta0` are the hyperparameters we are trying to estimate.  One technique for estimating is using maximum likelihood, and another is based on population mean and variance. 

```{r echo=TRUE}
career_filtered <- career %>% filter(AB > 500)

estimate_beta_parameters <- function(mu, var) {
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
  return(params = list(alpha = alpha, beta = beta))
}

average_mu = mean(career_filtered$average)
average_var = var(career_filtered$average)

parms <- estimate_beta_parameters(average_mu, average_var)
parms
```

We can use these parameters to create an estimated batting average.

```{r echo=TRUE}
career_eb <- career %>% mutate(eb_estimate = (H + parms$alpha) / (AB + parms$alpha + parms$beta))
```

Looking more closely at the results, the best and words are not merely the players with small numbers of bats with, particularly good or bad records.

```{r echo=TRUE}
career_eb %>% dplyr::arrange(eb_estimate)
```

```{r echo=TRUE}
career_eb %>% dplyr::arrange(desc(eb_estimate))
```

### 3.4 Visualizing The Results

Moving our estimates towards an average is often called `shrinkage.` _Extraordinatry outliers require extraordinary evidence._

```{r echo=TRUE}
career_eb %>% 
          ggplot(aes(x = average, y = eb_estimate, color = AB)) + 
          geom_point() +
          xlab("Batting Average") +
          ylab("Empirical Bayes Batting Average")
```

### 3.5 So comfortable it feels like cheating

The steps for using EB are:

1. Estimate the overall distribution of your data
2. Use that distribution as a prior for estimating your average

> "empirical Bayesian shrinkage towards a Beta prior."

## 4. Credible Intervals

Rather than estimating the 'best guess,' we want to know how much uncertainty is in our point estimates.  In this case, we are talking about a `credible interval.`

### 4.2 Posterior distribution

We are calculating a new posterior distribution for each player based on the update rules.

```{r echo=TRUE}
# values estimated by maximum likelihood in Chapter 3
alpha0 <- 101.4 
beta0  <- 287.3

# calculate our `point estimate` applying shrinkage to a beta prior
career_eb <- career %>% 
                    mutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0)) %>% 
                    mutate(alpha1 = alpha0 + H, beta1 = beta0 + AB - H)

# apply our update rules to calculate a new posterior beta distribution for every player
sampled_players <- career_eb %>% sample_n(7)
```

### 4.3 Credible intervals

A credible interval means that some percentage (say 95%) lies within a particular region.  Consider the plot below, showing Derek Jeter's 95% Confidence Interval.

```{r jeterinterval"}
# Adapted from https://github.com/dgrtwo/empirical-bayes-book/blob/master/credible-intervals.Rmd#L69-L88
jeter <- career_eb %>%
  filter(name == "Derek Jeter") %>%
  crossing(x = seq(.18, .33, .0002)) %>%
  ungroup() %>%
  mutate(density = dbeta(x, alpha1, beta1))
 
jeter_pred <- jeter %>%
  mutate(cumulative = pbeta(x, alpha1, beta1)) %>%
  filter(cumulative > .025, cumulative < .975)

jeter_low <- qbeta(.025, jeter$alpha1[1], jeter$beta1[1])
jeter_high <- qbeta(.975, jeter$alpha1[1], jeter$beta1[1])
jeter %>%
  ggplot(aes(x, density)) +
  geom_line() +
  geom_ribbon(aes(ymin = 0, ymax = density), data = jeter_pred,
              alpha = .25, fill = "red") +
  stat_function(fun = function(x) dbeta(x, alpha0, beta0),
                lty = 2, color = "black") +
  geom_errorbarh(aes(xmin = jeter_low, xmax = jeter_high, y = 0), height = 3.5, color = "red") +
  xlim(.18, .34)
```


> You can compute the edges of the credible interval quite easily using the [qbeta](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/Beta.html) (quantile of Beta) function in R. We just provide it the posterior `alpha1` and `beta1` parameters for each player. [Source: GitHub](https://github.com/dgrtwo/empirical-bayes-book/blob/master/credible-intervals.Rmd#L121)

```{r echo=TRUE}
sampled_players <- sampled_players %>%
  mutate(low  = qbeta(.025, alpha1, beta1),
         high = qbeta(.975, alpha1, beta1))

sampled_players %>% unique()
```

### 4.4 Credible Intervals and confidence intervals

Distinguishing between Confidence Intervals and Credible Intervals, let's assume we have chosen a confidence level and a credible interval of 95%.  How we would interpret these would differ as such:

- Confidence Interval (Frequentist):
  - Out of 100 experiments, at least 95 of those experiments would be expected to provide confidence intervals containing true value. The other five could be wrong, or nonsense, but that is ok, we care about the 95.
- Credible Interval (Bayesian):
  - The true value of our parameter is unknown. However, we have established a probability density estimate that represents our belief about that true value. A 95% credible interval will include 95% of the probability distribution that we have estimated for that estimated parameters

Credible and confidence intervals start looking more and more identical when:

1. We have a larger volume of observations, providing more information (large n),
2. the prior is less informative (small alpha0 and beta0)

---

## 5. Hypothesis Testing and FDR

`False Discovery Rate` is a mechanism for conceptualizing the rate of type 1 errors (i.e., False Positive).

```{r}
# https://github.com/dgrtwo/empirical-bayes-book/blob/master/hypothesis-testing.Rmd#L30-L54
library(dplyr) 
library(tidyr) 
library(Lahman)

career <- Batting %>%
  filter(AB > 0) %>%
  anti_join(Pitching, by = "playerID") %>% 
  group_by(playerID) %>%
  summarize(H = sum(H), AB = sum(AB)) %>% 
  mutate(average = H / AB)

career <- Master %>%
  tbl_df() %>%
  dplyr::select(playerID, nameFirst, nameLast) %>% 
  unite(name, nameFirst, nameLast, sep = " ") %>% 
  inner_join(career, by = "playerID")
# values estimated by maximum likelihood in Chapter 3
alpha0 <- 101.4 
beta0 <- 287.3
career_eb <- career %>%
  mutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0),
  alpha1 = H + alpha0, beta1 = AB - H + beta0)
```

### 5.2 Posterier Error Probabilities

> Consider the legendary player Hank Aaron. His career batting average is 0.3050, but we'd like to base our Hall of Fame admission on his "true probability" of hitting. Should he be permitted in our >.300 Hall of Fame?


```{r echo=TRUE}
career_eb %>%
  filter(name == "Hank Aaron")
```

```{r echo=TRUE}
# https://github.com/dgrtwo/empirical-bayes-book/blob/master/hypothesis-testing.Rmd#L70-L81
career_eb %>%
  filter(name == "Hank Aaron") %>%
  do(tibble(x = seq(.27, .33, .0002),
                density = dbeta(x, .$alpha1, .$beta1))) %>%
  ggplot(aes(x, density)) +
  ggtitle("Visualizing the `PEP` for Hank Aaron for a batting average of `0.300`") +
  geom_line() +
  geom_ribbon(aes(ymin = 0, ymax = density * (x < .3)),
              alpha = .1, fill = "red") +
  geom_vline(color = "red", lty = 2, xintercept = .3) +
  labs(x = "Batting average")
```

Given our selected threshold of `0.300`, we can use a cumulative distribution function to calculate Hank Aaron's true batting average below 0.3 (see plot above).  The probability that his true batting average is less than `0.300` is called the `Posterior Error Probability` or `PEP.`  As the shrunken batting average approaches our threshold of `0.300`, the `PEP` approaches `0.5`.  This is because the shrunken batting-average is the center point of the distribution.  For example, if the shrunken batting average of a player were `0.500`, exactly half of their updated beta distribution would be below `0.300`, and one half would be above.  Take Manny Mota, for example, his shrunken batting average rounds to 0.3 when rounded to 4 significant digits, let's visualize his `PEP.`

```{r echo=TRUE}
manny_mota <- career_eb %>%
  filter(round(eb_estimate, 4) == 0.3)

manny_mota
```


```{r echo=TRUE}
manny_mota %>%
  do(tibble(x = seq(.27, .33, .0002),
                density = dbeta(x, .$alpha1, .$beta1))) %>%
  ggplot(aes(x, density)) +
  ggtitle("Visualizing the `PEP` for Manny Mota for a batting average of `0.300`") +
  geom_line() +
  geom_ribbon(aes(ymin = 0, ymax = density * (x < .3)),
              alpha = .1, fill = "red") +
  geom_vline(color = "red", lty = 2, xintercept = .3) +
  labs(x = "Batting average")
```

The probability that Manny's actual batting average is less than `0.300`, we can use our `PEP` calculation, with our updated `alpha` and `beta` values to calculate it with the `pbeta` function.

```{r echo=TRUE}
manny_PEP <- pbeta(0.3, manny_mota$alpha1[1], manny_mota$beta1[1])
manny_PEP
```

So, building on the intuition, the estimated batting average represents the middle range of possible true batting averages.  As Manny's batting average sits nearly centered on our decision threshold of `0.300`, we can see that `0.5005843` sits below the threshold and the remainder sites above.  

### 5.3 False Discovery Rate

> Now, we want to set some threshold for inclusion in our Hall of Fame. This criterion is up to us: what kind of goal do we want to set? There are many options, but I'll propose one common in statistics: let's try to include as many players as possible while ensuring that no more than 5% of the Hall of Fame was mistakenly included. Put another way; we want to ensure that if you're in the Hall of Fame, the probability you belong is at least 95%.
> This criterion is called false **discovery rate control. It's particularly relevant in scientific studies, where we might want to come up with a set of candidates (e.g., genes, countries, individuals) for future study. There's nothing special, about 5%: if we wanted to be more strict, we could choose the same policy, but change our desired FDR to 1% or .1%. Similarly, if we wanted a broader set of candidates to study, we could set an FDR of 10% or 20%.
<https://github.com/dgrtwo/empirical-bayes-book/blob/master/hypothesis-testing.Rmd#L125-L127>

So, suppose `PEP` represents the probability that a batter's true average is less than our threshold. We can begin by examining players with high values for their estimated batting average and low PEPs.  We can start with observations whose posterior distributions are above our threshold while minimizing our `PEP`.

```{r echo=TRUE}
career_eb <- career_eb %>% mutate(PEP = pbeta(.3, alpha1, beta1))

top_players <- career_eb %>% arrange(PEP) %>% head(100)

top_players
```

>Well, we know the PEP of each of these 100 players, which is the probability that that individual player is a false positive. This means we can just add up these probabilities to get the expected value.

```{r echo=TRUE}
sum(top_players$PEP)
```

This suggests we should expect 6 of our 100 players to have a batting average less than `0.300`.  

> Note that we're calculating the FDR as 6.07/100 = 6.07%. Thus, we're computing the mean PEP: the average Posterior Error Probability.

```{r echo=TRUE}
mean(top_players$PEP)
```

## 5.4 Q-Values

The `False Discovery Rate` is above our desired threshold, so 100 players are too many, we could go through an iterative process of picking various player counts until we get an `FDR` we are comfortable with, OR, we could use the concept of a cumulative mean of all the (sorted) posterior error probabilities or `q-value` to help us pick our threshold.

We arrange our observations in descending order of PEP and then calculate our `q-value` as the running mean of our `posterior error probability`.  We then use that `q-value` to pick the cutoff point for our dataset that meets our `FDR` threshold

```{r echo=TRUE}
career_eb <- career_eb %>% 
  dplyr::arrange(PEP) %>% 
  dplyr::mutate(qvalue = dplyr::cummean(PEP))
```

> The q-value is convenient because we can say "to control the FDR at X%, collect only hypotheses where q < X".

```{r echo=TRUE}
hall_of_fame <- career_eb %>% filter(qvalue < .05)
count(hall_of_fame)
```

In this case, at a 5% `FDR`, we would only include 94 players in the hall of fame.  We can change our threshold and see how that affects entry into the hall of fame.

```{r echo=TRUE}
strict_hall_of_fame <- career_eb %>% filter(qvalue < .01)
count(strict_hall_of_fame)
```

Increasing the threshold to 1% acceptable `FDR` lowers the number of players we can let into only 61 players. Consider the following plot, as we increase the acceptable `q-value` we increase the number of players we would allow into our hall-of-fame.

```{r echo =TRUE}
career_eb %>%
  filter(qvalue < .3) %>%
  ggplot(aes(qvalue, rank(PEP))) +
  geom_line() +
  scale_x_continuous(labels = scales::percent_format()) +
  xlab("q-value threshold") +
  ylab("Number of players included at this threshold")
```

To reiterate and contrast.

- `PEP` represents the probability than an individual observation is a false positive
- `q-value` represents the expected false discovery rate for all included observations including the individual (average of PEP for obersvations)

## 6. Bayesian A/B Testing

Which option is better?  That is a common request 

We want to know _the probability that one beta distribution is greater than another_.  There are four common approaches.

1. Simulation of posterier draws
  - sample both distributions many times over, calculate the fraction of time where one option is greater than the other.
2. Numerical integration
  - Create a joint distribution, determine the fraction of that joint distribution that resides with one option over the other
3. Closed-form solution
4. Closed-form approxiamtions


### 6.2 Comparing Posterier Distributions

### 6.2.1 Simulation of posterier draws
### 6.2.2 Numerical Integration
### 6.2.1 Closed-form solution
### 6.2.1 Closed-form approximation


### 6.3 Confidence and credible intervals

