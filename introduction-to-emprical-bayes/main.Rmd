---
title: "Introduction To Empirical Bayes Notes"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
renv::restore()
r_packages <- c("ggplot2", "dplyr", "tidyr", "purrr")
renv::install(r_packages)

library(dplyr)
library(tidyr)
library(ggplot2)
library(purrr)
ggplot2::theme_set(ggplot2::theme_bw())
```

Notes from [Introduction to Empirical Bayes: Examples from Baseball Statistics](http://varianceexplained.org/r/empirical-bayes-book/) by David Robinson

## Why Empirical Bayes?

Empirical Bayesian methods are an approximation to more exact methods, but come with some controversy. However, EB methods are well suited for Data Science because

- Though EB is Inacurate with few observations, with large datasets performance can be comperable to traditional Baysiean methods
- EB provides shortcuts that allow easy computation at scale; Approximate in a fraction of the time of full Baysien Methos

We can use EB as a fast approximation technique, when we have many observations.

## 2 The beta distribution

The _beta distribtion_ is a probability distribution with two Parameters $\alpha$ (alpha) and $\beta$ (beta).  The distribution is constrained between 0 an 1.  In practice, it is good at representing a probabilty distribution of probabilities.

```{r, echo = FALSE, assorted-betas}
size = 1e6
tibble::tibble(
  alpha = c(1, 3, 20, 50),
  beta  = c(2, 3, 20, 10),
) %>% dplyr::mutate(
  batting_average  = purrr::map2(alpha, beta, ~ rbeta(size, .x, .y)),
  Parameters = paste0("alpha = ", alpha, " beta = ", beta)
) %>% 
  tidyr::unnest(batting_average) %>%
  ggplot2::ggplot(ggplot2::aes(x = batting_average, color = Parameters)) + 
  ggplot2::geom_density() + 
  ggplot2::xlab("Batting Average") +
  ggplot2::ylab("Density of beta") + 
  theme_bw()
```

### 2.1 The Binomial Distribution

The _binomial distribution_ models a count of successes out of the total.  To follow the baseball analogy, for 100 at-bats we would say the number of hits a player will get is distribted according to `Binomial(100, p)` where `p` is the probability that at-bat `AB` will be a hit `H`.  In the baseball case, the `p` the thing we want estimate is a batting average.

> Batting Average = Hits / At Bats

### 2.2 Updating The Beta Distribution

We can build an existing belief (prior) regarding what a reasonable batting average should be.  The text suggests that belief is 0.27, with an alpha = 81, and beta of 219.  However, a batting average could reasonably range from 0.21 to 0.35 (see plot below).  So if we start with this existing belief of where any players true batting average _could be_, how do we update our prior beliefs based on new information as the player gets at-bats in.

```{r, echo = FALSE, beta_for_batting_averages}
size = 1e6
tibble::tibble(
  alpha = c(81),
  beta  = c(219),
) %>% dplyr::mutate(
  batting_average  = purrr::map2(alpha, beta, ~ rbeta(size, .x, .y)),
  Parameters = paste0("alpha = ", alpha, " beta = ", beta)
) %>% 
  tidyr::unnest(batting_average) %>%
  ggplot2::ggplot(ggplot2::aes(x = batting_average, color = Parameters)) + 
  ggplot2::geom_density() + 
  ggplot2::xlab("Batting Average") +
  ggplot2::ylab("Density of beta") + 
  theme_bw()
```

This idea, `prior expectations + new information = new expectation` is a simplificiation of the Bayesian Philosopy.  In the case of updating our beta distribution, the update rule is very simple.

> Beta(alpha<sub>0</sub> + hits, beta<sub>0</sub> + misses)

So, if a player were to have a hit on their first at-bat, the new beta would be be Beta(82, 219).  When we plot both distributions we can see that the posterior distribution shifts slightly to the right.  The movement is very small, however, if we were to consider a batter who has hit 200 out of 600 at bats, the new posterier would be be Beta(282, 619), a much bigger shift, since we have collected more evidence for the update rule.

```{r, echo = FALSE, update-the-betas-a-little-and-a-lot}
size = 1e6
tibble::tibble(
  alpha = c(81, 82, 282),
  beta  = c(219, 219, 619),
) %>% dplyr::mutate(
  batting_average  = purrr::map2(alpha, beta, ~ rbeta(size, .x, .y)),
  Parameters = paste0("alpha = ", alpha, " beta = ", beta)
) %>% 
  tidyr::unnest(batting_average) %>%
  ggplot2::ggplot(ggplot2::aes(x = batting_average, color = Parameters)) + 
  ggplot2::geom_density() + 
  ggplot2::xlab("Batting Average") +
  ggplot2::ylab("Density of beta") + 
  theme_bw()
```

### 2.2.1 Calculating The Posterier Mean

The expected mean of the Beta distribution can be described as:

> alpha / (alpha + beta)

## 2.3 Conjugate Prior

Why is the update so easy?  It is becuase the beta distribution is the [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior#Example) of the Binomial distribution.  Which means there are clear cut rules on how to update the prior to get to the posterier distribution. [Wikipedia has a list](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions).  If a prior is not a conjugate pior, then the update can get messy.


Consider a player who got exactly 100 hits in the first 300 at bats.  What is the true value of their batting average?  We can get a better sense of possible values by simulating many players based on our prior belief regarding the distribution of batting averages we start with Beta(81, 219).


```{r}
library(dplyr)
num_trials <- 10e6

# Simulate batting statistica based on our existing Prior for our Beta
# Get to a count using the binomial and the true average
simulations <- tibble::tibble(
  true_average = rbeta(num_trials, 81, 219),
  hits = rbinom(num_trials, 300, true_average)
)

# Consider the player who got exactly 100 hits, what is the expected range of their true batting average?
simulations %>% 
            dplyr::filter(hits == 100) %>% 
            ggplot2::ggplot(aes(x=true_average)) + 
                     geom_bar(stat="bin", binwidth = 0.004, ggplot2::aes(y=..density..)) + 
                     geom_density()  + 
                     xlab("Batting average of players who got 100 H / 300 AB.")
                     ggplot2::theme_bw()
```

What if our player had 60, 80 or 100 hits instead.  The simulations would look like plot below.  The shapes are simliar, but they are shifted to accomodate our new information.  The simulations are not required long term, but helpful for demonstration.  Eventually we will rely on the update rule "add `hits` to `alpha`, and add `misses` to `beta`"

```{r}
simulations %>%
  filter(hits %in% c(60, 80, 100)) %>% ggplot(aes(true_average, color = factor(hits))) + geom_density() +
  labs(x = "True average of players with H hits / 300 at-bats",
  color = "H") +                
  ggplot2::theme_bw()
```

## 2. Empirical Bayes Estimation

Which sounds higher, `4 out of 10`, or `300 out of 1000`?  Obviously 4/10 = 0.4 and 300/1000 = 0.3.  `0.4` > `0.3` so the answer is clear, but 10 AB is not nearly as much evidence as 1000 AB, so we would have different levels of confidence regarding each.  This sort of hits / attempts analysis is not limited to baseball, consider online advertising for a simliar "proportion of success" analysis.

### 2.1 The Lahman baseball dataset

Let's look at real baseball stats wth `Lahman`.


```{r}
library(dplyr) 
library(tidyr) 
library(Lahman)
# Filter out pitchers
career <- Batting %>%
                  filter(AB > 0) %>%
                  anti_join(Pitching, by = "playerID") %>% 
                  group_by(playerID) %>%
                  summarize(H = sum(H), AB = sum(AB)) %>% 
                  mutate(average = H / AB) %>%
                  tibble::as_tibble()

# Include names along with the player IDs
career <- Master %>%
            tibble::as_tibble() %>%
            dplyr::select(playerID, nameFirst, nameLast) %>% 
            unite(name, nameFirst, nameLast, sep = " ") %>% 
            inner_join(career, by = "playerID") %>% 
            dplyr::select(-playerID)

career
```

## 3.2 Estimating A Prior From All Your Data

Our first step of empirical Baeys estimation is to esitmate a beta prior, using the data.  Debate exists if an when it is appropriate to use Empirical Bayes methods, but it basically comes down to how many observations we have.  The less dependent we are on a single observation, the better.  Ultimately, Empircal Bayes is an **approximation** of more exeact Baysian methods.  If we have a lot of data, it is a good approximation.

```{r}
career %>% 
  dplyr::filter(AB > 500) %>%
  ggplot2::ggplot(aes(x=average)) + 
  geom_histogram(bins = 40) +
  ggtitle("Batting Averages of all players with more than 500 At-Bats.")
```

We need to fit a model such that we fix a beta distribution to this dataset.  We can think of the model we want to fit as

`X ~ Beta(alpha0, beta0)` where `alpha0` and `beta0` are the hyperparamters we are trying to estimate.  One technique for estimating is using maximum likelihood, another is based on population mean and variance


```{r fitting-alpha0-and-beta0}
career_filtered <- career %>% filter(AB > 500)

estimate_beta_parameters <- function(mu, var) {
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
  return(params = list(alpha = alpha, beta = beta))
}

average_mu = mean(career_filtered$average)
average_var = var(career_filtered$average)

parms <- estimate_beta_parameters(average_mu, average_var)
parms
```

We can use these paramters to create an estimated batting average.

```{r}
career_eb <- career %>% mutate(eb_estimate = (H + parms$alpha) / (AB + parms$alpha + parms$beta))
```

Looking more closely at the results, the best and words are not simply the players with small numbers of at bats with particularly good or bad records.

```{r}
career_eb %>% dplyr::arrange(eb_estimate)
```

```{r}
career_eb %>% dplyr::arrange(desc(eb_estimate))
```

### 3.4 Visulizing The Results

Moving our estimates towards an average is often called `shrinkage`. _Extraordinatry outliers require extraordinary evidence._

```{r}
career_eb %>% 
          ggplot(aes(x = average, y = eb_estimate, color = AB)) + 
          geom_point() +
          xlab("Batting Average") +
          ylab("Empirical Bayes Batting Average")
```

## 3.5 So easy it feels like cheating

The steps for using EB are:

1. Estimate overall distribution of your data
2. User that distribution as a prior for estimating your average

> "empirical Bayesian shrinkage towards a Beta prior"


## 4

